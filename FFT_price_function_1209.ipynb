{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "from numpy import fft\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import math\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock_name, date_predict_start, data_range, slide_range, n_slide):\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    date_predict_start = datetime.datetime.strptime(\n",
    "        date_predict_start, '%Y-%m-%d')\n",
    "    date_data_start_list = []\n",
    "    date_predict_start_list = []\n",
    "    date_predict_end_list = []\n",
    "    for i in range(n_slide*2):\n",
    "        date_data_start = date_predict_start - \\\n",
    "            relativedelta(days=+data_range)\n",
    "        date_predict_end = date_predict_start + \\\n",
    "            relativedelta(days=+data_range)\n",
    "        date_data_start_list.append(date_data_start)\n",
    "        date_predict_start_list.append(date_predict_start)\n",
    "        date_predict_end_list.append(date_predict_end)\n",
    "        date_data_start = date_data_start + \\\n",
    "            relativedelta(days=+slide_range)\n",
    "        date_predict_start = date_predict_start + \\\n",
    "            relativedelta(days=+slide_range)\n",
    "\n",
    "    train_data_all = yf.Ticker(stock_name).history(\n",
    "        start=date_data_start_list[0], end=date_predict_start_list[-1])\n",
    "    test_data_all = yf.Ticker(stock_name).history(\n",
    "        start=date_predict_start_list[0], end=date_predict_end_list[-1])\n",
    "    test_data_all['count'] = range(len(test_data_all))\n",
    "    test_data_start_list = []\n",
    "    for i in range(n_slide):\n",
    "        train_data['data_' + str(i)] = train_data_all.iloc[i *\n",
    "                                                           slide_range:i*slide_range+data_range]\n",
    "        train_data['data_' + str(i)] = train_data['data_' +\n",
    "                                                  str(i)].reset_index(drop=True)\n",
    "        test_data['data_' + str(i)] = test_data_all.iloc[i *\n",
    "                                                         slide_range:i*slide_range+data_range]\n",
    "        test_data_start_list.append(test_data['data_' + str(i)].index[0])\n",
    "        test_data['data_' + str(i)] = test_data['data_' +\n",
    "                                                str(i)].reset_index(drop=True)\n",
    "    return train_data, test_data, test_data_all, test_data_start_list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_pv_CL_function(data, pv_range):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    for i in data:\n",
    "        pv = data[i]['Close']\n",
    "        data[i]['peaks'] = pd.Series(dtype='float64')\n",
    "        data[i]['valleys'] = pd.Series(dtype='float64')\n",
    "        peaks = data[i]['peaks']\n",
    "        valleys = data[i]['valleys']\n",
    "        for idx in range(0, len(pv)):\n",
    "            if idx < pv_range:\n",
    "                if pv[idx] == pv.iloc[0:pv_range*2+1].max():\n",
    "                    peaks.iloc[idx] = pv[idx]\n",
    "                if pv[idx] == pv.iloc[0:pv_range*2+1].min():\n",
    "                    valleys.iloc[idx] = pv[idx]\n",
    "            if pv[idx] == pv.iloc[idx-pv_range:idx+pv_range].max():\n",
    "                peaks.iloc[idx] = pv[idx]\n",
    "            if pv[idx] == pv.iloc[idx-pv_range:idx+pv_range].min():\n",
    "                valleys.iloc[idx] = pv[idx]\n",
    "        data[i]['peaks'] = peaks\n",
    "        data[i]['valleys'] = valleys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_pv_HL_function(data, pv_range):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    for i in data:\n",
    "        p = data[i]['High']\n",
    "        v = data[i]['Low']\n",
    "        data[i]['peaks'] = pd.Series(dtype='float64')\n",
    "        data[i]['valleys'] = pd.Series(dtype='float64')\n",
    "        peaks = data[i]['peaks']\n",
    "        valleys = data[i]['valleys']\n",
    "        for idx in range(0, len(p)):\n",
    "            if idx < pv_range:\n",
    "                if p[idx] == p.iloc[0:pv_range*2+1].max():\n",
    "                    peaks.iloc[idx] = p[idx]\n",
    "                if v[idx] == v.iloc[0:pv_range*2+1].min():\n",
    "                    valleys.iloc[idx] = v[idx]\n",
    "            if p[idx] == p.iloc[idx-pv_range:idx+pv_range].max():\n",
    "                peaks.iloc[idx] = p[idx]\n",
    "            if v[idx] == v.iloc[idx-pv_range:idx+pv_range].min():\n",
    "                valleys.iloc[idx] = v[idx]\n",
    "        data[i]['peaks'] = peaks\n",
    "        data[i]['valleys'] = valleys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(train_data, test_data, pv_range, pv_method):\n",
    "    if pv_method == 'CL':\n",
    "        find_data_pv_CL_function(train_data, pv_range)\n",
    "        find_data_pv_CL_function(test_data, pv_range)\n",
    "    elif pv_method == 'HL':\n",
    "        find_data_pv_HL_function(train_data, pv_range)\n",
    "        find_data_pv_HL_function(test_data, pv_range)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_harmonics_function(data_stock):\n",
    "    harmonics = {}\n",
    "    for i in data_stock:\n",
    "        harmonics[i] = {}\n",
    "        # get data_stock's infomation\n",
    "        data = data_stock[i]['Close']\n",
    "        array_data = np.array(data)\n",
    "        n_data = array_data.size\n",
    "        time_data = np.arange(0, n_data)\n",
    "\n",
    "        # detrend data\n",
    "        # find linear trend in data\n",
    "        Polynomial = np.polyfit(time_data, array_data, 1)\n",
    "        data_notrend = array_data - Polynomial[0] * time_data    # detrended x\n",
    "\n",
    "        # fft process\n",
    "        data_freqdom = fft.fft(data_notrend, n=n_data)\n",
    "        frequence = fft.fftfreq(n=n_data, d=1)\n",
    "        f_positive = frequence[np.where(frequence > 0)]\n",
    "        data_freqdom_positive = data_freqdom[np.where(frequence > 0)]\n",
    "\n",
    "        # sort indexes\n",
    "        indexes = list(range(f_positive.size))      # frequencies\n",
    "        # sort method 1\n",
    "        # indexes.sort(key = lambda i: np.absolute(frequence[i]))     # sort indexes by frequency, lower -> higher\n",
    "        # sort method 2 :\n",
    "        # sort indexes by amplitudes, lower -> higher\n",
    "        indexes.sort(key=lambda i: np.absolute(data_freqdom[i]))\n",
    "        indexes.reverse()       # sort indexes by amplitudes, higher -> lower\n",
    "\n",
    "        # get data_all_time'size\n",
    "        time_transfer = np.arange(0, 2*array_data.size)\n",
    "\n",
    "        # mix harmonics\n",
    "        for j in indexes:\n",
    "            ampli = np.absolute(\n",
    "                data_freqdom_positive[j]) / n_data     # amplitude\n",
    "            phase = np.angle(data_freqdom_positive[j])      # phase\n",
    "            harmonics[i][j] = ampli * \\\n",
    "                np.cos(2 * np.pi * f_positive[j] * time_transfer + phase)\n",
    "    return harmonics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_harmonics_function(harmonics, n_harm_lower_limit, n_harm_upper_limit):\n",
    "    processed_signal = {}\n",
    "    for i in harmonics:\n",
    "        processed_signal[i] = {}\n",
    "        for n_harm in range(n_harm_lower_limit, n_harm_upper_limit+1):\n",
    "            mixed_harmonic = np.zeros(len(harmonics[i][0]))\n",
    "            for j in range(n_harm):\n",
    "                mixed_harmonic += harmonics[i][j]\n",
    "            processed_signal[i][n_harm] = pd.DataFrame(\n",
    "                {'Close': mixed_harmonic})\n",
    "    return processed_signal\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signal processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signal_pv_function(signal, pv_range):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    for i in signal:\n",
    "        for j in signal[i]:\n",
    "            pv = signal[i][j]['Close']\n",
    "            signal[i][j]['peaks'] = pd.Series(dtype='float64')\n",
    "            signal[i][j]['valleys'] = pd.Series(dtype='float64')\n",
    "            peaks = signal[i][j]['peaks']\n",
    "            valleys = signal[i][j]['valleys']\n",
    "            for idx in range(0, len(pv)):\n",
    "                if idx < pv_range:\n",
    "                    if pv[idx] == pv.iloc[0:pv_range*2+1].max():\n",
    "                        peaks.iloc[idx] = pv[idx]\n",
    "                    if pv[idx] == pv.iloc[0:pv_range*2+1].min():\n",
    "                        valleys.iloc[idx] = pv[idx]\n",
    "                if pv[idx] == pv.iloc[idx-pv_range:idx+pv_range].max():\n",
    "                    peaks.iloc[idx] = pv[idx]\n",
    "                if pv[idx] == pv.iloc[idx-pv_range:idx+pv_range].min():\n",
    "                    valleys.iloc[idx] = pv[idx]\n",
    "            signal[i][j]['peaks'] = peaks\n",
    "            signal[i][j]['valleys'] = valleys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pv_lead_function(data, processed_signal):\n",
    "    for d in data:\n",
    "        for p in processed_signal[d]:\n",
    "            processed_signal[d][p]['pv'] = pd.Series(dtype='str')\n",
    "            processing_signal = processed_signal[d][p].loc[list(data[d].index)]\n",
    "            p_data = pd.DataFrame(\n",
    "                {'peaks': data[d]['peaks'], 'count': range(len(data[d]))})\n",
    "            p_data = p_data.drop(p_data[p_data['peaks'].isna()].index)\n",
    "            p_data_count = list(p_data['count'])\n",
    "            p_signal = pd.DataFrame(\n",
    "                {'peaks': processing_signal['peaks'], 'count': range(len(processing_signal))})\n",
    "            p_signal = p_signal.drop(p_signal[p_signal['peaks'].isna()].index)\n",
    "            p_signal_list = list(p_signal['count'])\n",
    "            p_lead = []\n",
    "            for i in range(0, len(p_signal_list)):\n",
    "                temp = []\n",
    "                temp_abs = []\n",
    "                temp_2 = []\n",
    "                for j in range(0, len(p_data_count)):\n",
    "                    temp.append((p_data_count[j] - p_signal_list[i]))\n",
    "                    temp_abs.append(abs(p_data_count[j] - p_signal_list[i]))\n",
    "                for k in range(0, len(temp_abs)):\n",
    "                    if temp_abs[k] == min(temp_abs):\n",
    "                        temp_2 = temp[k]\n",
    "                p_lead.append(temp_2)\n",
    "            p_signal['lead'] = p_lead\n",
    "\n",
    "            v_data = pd.DataFrame(\n",
    "                {'valleys': data[d]['valleys'], 'count': range(len(data[d]))})\n",
    "            v_data = v_data.drop(v_data[v_data['valleys'].isna()].index)\n",
    "            v_data_count = list(v_data['count'])\n",
    "            v_signal = pd.DataFrame(\n",
    "                {'valleys': processing_signal['valleys'], 'count': range(len(processing_signal))})\n",
    "            v_signal = v_signal.drop(\n",
    "                v_signal[v_signal['valleys'].isna()].index)\n",
    "            v_signal_list = list(v_signal['count'])\n",
    "            v_lead = []\n",
    "            for i in range(0, len(v_signal_list)):\n",
    "                temp = []\n",
    "                temp_abs = []\n",
    "                temp_2 = []\n",
    "                for j in range(0, len(v_data_count)):\n",
    "                    temp.append((v_data_count[j] - v_signal_list[i]))\n",
    "                    temp_abs.append(abs(v_data_count[j] - v_signal_list[i]))\n",
    "                for k in range(0, len(temp_abs)):\n",
    "                    if temp_abs[k] == min(temp_abs):\n",
    "                        temp_2 = temp[k]\n",
    "                v_lead.append(temp_2)\n",
    "            v_signal['lead'] = v_lead\n",
    "\n",
    "            processed_signal[d][p]['lead'] = pd.Series(dtype='float64')\n",
    "            processed_signal[d][p]['lead'].loc[p_signal['lead'].index] = p_signal['lead']\n",
    "            processed_signal[d][p]['pv'].loc[p_signal['lead'].index] = 'peak'\n",
    "            processed_signal[d][p]['lead'].loc[v_signal['lead'].index] = v_signal['lead']\n",
    "            processed_signal[d][p]['pv'].loc[v_signal['lead'].index] = 'valley'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train_data, n_harm_lower_limit, n_harm_upper_limit, pv_range):\n",
    "    harmonics = data_to_harmonics_function(train_data)\n",
    "    processed_signal = mix_harmonics_function(\n",
    "        harmonics, n_harm_lower_limit, n_harm_upper_limit)\n",
    "    find_signal_pv_function(processed_signal, pv_range)\n",
    "    find_pv_lead_function(train_data, processed_signal)\n",
    "    return harmonics, processed_signal\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fit_error_function(processed_signal, fit_method):\n",
    "    errors = {}\n",
    "    error = []\n",
    "    for i in processed_signal:\n",
    "        errors[i] = {}\n",
    "        for j in processed_signal[i]:\n",
    "            signal_dropna = processed_signal[i][j].drop(\n",
    "                processed_signal[i][j][processed_signal[i][j]['lead'].isna()].index)\n",
    "            if fit_method == 'mean':\n",
    "                error = signal_dropna['lead'].mean()\n",
    "            elif fit_method == 'abs':\n",
    "                error = abs(signal_dropna['lead']).mean()\n",
    "            elif fit_method == 'rmse':\n",
    "                mse = np.square(np.subtract(np.zeros_like(signal_dropna['lead']),signal_dropna['lead'])).mean() \n",
    "                rmse = math.sqrt(mse)\n",
    "                error = rmse\n",
    "            errors[i][j] = error\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_fit_harm_function(processed_signal, errors):\n",
    "    best_error = {}\n",
    "    best_fit_harm = {}\n",
    "    for i in processed_signal:\n",
    "        best_error[i] = pd.Series(errors[i]).abs().min()\n",
    "        best_fit_harm[i] = pd.Series(errors[i]).abs().idxmin()\n",
    "    return best_fit_harm, best_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_lead_function(processed_signal, best_fit_harm):\n",
    "    first_date = {}\n",
    "    lead = {}\n",
    "    pv = {}\n",
    "    for i in processed_signal:\n",
    "        harm = best_fit_harm[i]\n",
    "        temp = processed_signal[i][harm].loc[list(\n",
    "            processed_signal[i][harm]['lead'].dropna().index)[0]]\n",
    "        first_date[i] = list(processed_signal[i][harm]\n",
    "                             ['lead'].dropna().index)[0]\n",
    "        lead[i] = temp['lead']\n",
    "        pv[i] = temp['pv']\n",
    "    return first_date, lead, pv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(processed_signal, fit_method):\n",
    "    errors = get_fit_error_function(processed_signal, fit_method)\n",
    "    best_fit_harm, best_error = get_best_fit_harm_function(\n",
    "        processed_signal, errors)\n",
    "    first_date, lead, pv = get_first_lead_function(\n",
    "        processed_signal, best_fit_harm)\n",
    "    return errors, best_fit_harm, best_error, first_date, lead, pv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_result_table_function(processed_signal, test_data_start_list, lead, pv, best_error, best_fit_harm):\n",
    "    result_table = pd.DataFrame(columns=[\n",
    "        's_date', 't_date', 'lead', 'ans_date', 'pv', 'error', 'best_fit'])\n",
    "    for i in processed_signal:\n",
    "        result_table.loc[i, 'error'] = round(best_error[i], 2)\n",
    "        result_table.loc[i, 'best_fit'] = best_fit_harm[i]\n",
    "        result_table.loc[i, 'lead'] = lead[i]\n",
    "        result_table.loc[i, 'pv'] = pv[i]\n",
    "    result_table['s_date'] = test_data_start_list\n",
    "    return result_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_table_process_function(result_table, test_data_all, first_date):\n",
    "    for i in result_table.index:\n",
    "        t_date = test_data_all.loc[test_data_all['count'] ==\n",
    "                                   test_data_all['count'].loc[result_table.loc[i, 's_date']] +\n",
    "                                   first_date[i]].index[0]\n",
    "        t_date = datetime.datetime.strftime(t_date, '%Y-%m-%d')\n",
    "        result_table.loc[i, 't_date'] = t_date\n",
    "        ans = test_data_all.loc[test_data_all['count'] == test_data_all['count']\n",
    "                                .loc[result_table.loc[i, 't_date']] + result_table.loc[i, 'lead']].index[0]\n",
    "        ans = datetime.datetime.strftime(ans, '%Y-%m-%d')\n",
    "        result_table.loc[i, 'ans_date'] = ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_error_function(result_table):\n",
    "    final_error = round(\n",
    "        sum([abs(ele) for ele in result_table['lead']]) / len(result_table['lead']), 2)\n",
    "    return final_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(processed_signal, test_data_start_list, test_data_all, best_fit_harm, best_error, first_date, lead, pv):\n",
    "    result_table = built_result_table_function(\n",
    "        processed_signal, test_data_start_list, lead, pv, best_error, best_fit_harm)\n",
    "    result_table_process_function(result_table, test_data_all, first_date)\n",
    "    final_error = compute_final_error_function(result_table)\n",
    "    return result_table, final_error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_funtion(\n",
    "    stock_name, date_predict_start, data_range, slide_range,\n",
    "        n_slide, pv_range, n_harm_lower_limit, n_harm_upper_limit, fit_method, pv_method):\n",
    "\n",
    "    # 1. Load data\n",
    "    train_data, test_data, test_data_all, test_data_start_list = load_data(\n",
    "        stock_name, date_predict_start, data_range, slide_range, n_slide)\n",
    "    # 2. Preprocessing\n",
    "    preprocessing(train_data, test_data, pv_range, pv_method)\n",
    "    \n",
    "    # 3. Build model\n",
    "    harmonics, model = build_model(\n",
    "        train_data, n_harm_lower_limit, n_harm_upper_limit, pv_range)\n",
    "    # 4. Select model\n",
    "    errors, best_fit_harm, best_error, first_date, lead, pv = select_model(\n",
    "        model, fit_method)\n",
    "\n",
    "    # 5. Evaluate model\n",
    "    result_table, final_error = evaluate_model(\n",
    "        model, test_data_start_list, test_data_all, best_fit_harm, best_error, first_date, lead, pv)\n",
    "    print('final_error = ', final_error)\n",
    "    print(result_table)\n",
    "    # 6. Predict\n",
    "\n",
    "    return harmonics, model, errors, best_fit_harm, best_error, first_date, lead, pv, result_table, final_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_error =  0.9\n",
      "           s_date      t_date lead    ans_date      pv error best_fit\n",
      "data_0 2018-12-31  2019-01-03 -1.0  2019-01-02    peak  0.89        5\n",
      "data_1 2019-01-08  2019-01-08  1.0  2019-01-09    peak  3.42        5\n",
      "data_2 2019-01-15  2019-01-17  0.0  2019-01-17    peak  2.24        5\n",
      "data_3 2019-01-23  2019-01-23  0.0  2019-01-23  valley  2.65        5\n",
      "data_4 2019-01-30  2019-01-31  5.0  2019-02-07    peak  2.27        5\n",
      "data_5 2019-02-06  2019-02-07  0.0  2019-02-07    peak   1.5        5\n",
      "data_6 2019-02-13  2019-02-15 -1.0  2019-02-14  valley  1.54        5\n",
      "data_7 2019-02-21  2019-02-25  0.0  2019-02-25    peak  0.53        5\n",
      "data_8 2019-02-28  2019-02-28  0.0  2019-02-28  valley  0.65        5\n",
      "data_9 2019-03-07  2019-03-11  1.0  2019-03-12  valley  0.82        5\n"
     ]
    }
   ],
   "source": [
    "stock_name = \"^GSPC\"\n",
    "date_predict_start = '2019-01-01'\n",
    "data_range = 20\n",
    "slide_range = 5\n",
    "n_slide = 10\n",
    "pv_range = 2\n",
    "n_harm_lower_limit = 5\n",
    "n_harm_upper_limit = 5\n",
    "fit_method = 'rmse'\n",
    "pv_method = 'HL'\n",
    "harmonics, model, errors, best_fit_harm, best_error, first_date, lead, pv, result_table, final_error=  main_funtion(\n",
    "    stock_name, date_predict_start, data_range, slide_range,\n",
    "    n_slide, pv_range, n_harm_lower_limit, n_harm_upper_limit, fit_method, pv_method)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv_stock': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba787645ba8ce3b43ca5d5c1bc5ea17dd580ada22e1bd31731a2a86b718f16cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
