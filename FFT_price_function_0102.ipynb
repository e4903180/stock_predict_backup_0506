{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "from numpy import fft\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import math\n",
    "import matplotlib.dates as mdates\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock_name, date_predict_start, data_range, slide_range, n_slide):\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    date_predict_start = datetime.datetime.strptime(\n",
    "        date_predict_start, '%Y-%m-%d')\n",
    "    date_data_start_list = []\n",
    "    date_predict_start_list = []\n",
    "    date_predict_end_list = []\n",
    "    test_data_start_list = []\n",
    "    all_data = yf.Ticker(stock_name).history(period='max')\n",
    "\n",
    "    check = date_predict_start in list(all_data.index)\n",
    "    while (check == False):\n",
    "        date_predict_start = date_predict_start + \\\n",
    "            relativedelta(days=+1)\n",
    "        check = date_predict_start in list(all_data.index)\n",
    "\n",
    "    predict_start = all_data.index.get_loc(date_predict_start)\n",
    "    for i in range(n_slide):\n",
    "        predict_end = predict_start + data_range\n",
    "        date_predict_end = all_data.iloc[predict_end].name\n",
    "        data_start = predict_start - data_range\n",
    "        date_data_start = all_data.iloc[data_start].name\n",
    "        train_data['data_' + str(i)] = all_data.iloc[data_start:predict_start]\n",
    "        test_data['data_' + str(i)] = all_data.iloc[predict_start:predict_end]\n",
    "        date_data_start_list.append(date_data_start)\n",
    "        date_predict_start_list.append(date_predict_start)\n",
    "        date_predict_end_list.append(date_predict_end)\n",
    "        test_data_start_list.append(datetime.datetime.strftime(\n",
    "            test_data['data_' + str(i)].index[0], '%Y-%m-%d'))\n",
    "        data_start = data_start + slide_range\n",
    "        predict_start = predict_start + slide_range\n",
    "        train_data['data_' + str(i)] = train_data['data_' +\n",
    "                                                  str(i)].reset_index(drop=False)\n",
    "        test_data['data_' + str(i)] = test_data['data_' +\n",
    "                                                str(i)].reset_index(drop=False)\n",
    "\n",
    "    return train_data, test_data, all_data, test_data_start_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_name = \"^GSPC\"\n",
    "# date_predict_start = '2021-01-01'\n",
    "# data_range = 10\n",
    "# slide_range = 5\n",
    "# n_slide = 3\n",
    "# pv_range = 2\n",
    "# n_harm_lower_limit = 3\n",
    "# n_harm_upper_limit = 3\n",
    "# fit_method = 'rmse'\n",
    "# pv_method = 'HL'\n",
    "# date_data_start_list = []\n",
    "# date_predict_start_list = []\n",
    "# date_predict_end_list = []\n",
    "# train_data = {}\n",
    "# test_data = {}\n",
    "# date_predict_start = datetime.datetime.strptime(date_predict_start, '%Y-%m-%d')\n",
    "# all_data = yf.Ticker(stock_name).history(period='max')\n",
    "\n",
    "# check = date_predict_start in list(all_data.index)\n",
    "# while (check == False) :\n",
    "#     date_predict_start = date_predict_start + \\\n",
    "#         relativedelta(days=+1)\n",
    "#     print(check)\n",
    "#     check = date_predict_start in list(all_data.index)\n",
    "# print(check)\n",
    "# print(date_predict_start)\n",
    "# predict_start = all_data.index.get_loc(date_predict_start)\n",
    "# for i in range(n_slide):\n",
    "#     predict_end = predict_start + data_range\n",
    "#     date_predict_end = all_data.iloc[predict_end].name\n",
    "#     data_start = predict_start - data_range\n",
    "#     date_data_start = all_data.iloc[data_start].name\n",
    "#     train_data['data_' + str(i)] = all_data.iloc[data_start:predict_start]\n",
    "#     test_data['data_' + str(i)] = all_data.iloc[predict_start:predict_end]\n",
    "#     date_data_start_list.append(date_data_start)\n",
    "#     date_predict_start_list.append(date_predict_start)\n",
    "#     date_predict_end_list.append(date_predict_end)\n",
    "#     data_start = data_start + slide_range\n",
    "#     predict_start = predict_start + slide_range\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_pv_CL_function(data, pv_range):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    for i in data:\n",
    "        price = data[i]['Close']\n",
    "        data[i]['peaks'] = pd.Series(dtype='float64')\n",
    "        data[i]['valleys'] = pd.Series(dtype='float64')\n",
    "        data[i]['pv'] = pd.Series(dtype='str')\n",
    "        for idx in range(0, len(price)):\n",
    "            if idx < pv_range:\n",
    "                if price[idx] == price.iloc[0:pv_range*2+1].max():\n",
    "                    data[i]['peaks'].iloc[idx] = price[idx]\n",
    "                    data[i]['pv'].iloc[idx] = 'peaks'\n",
    "                if price[idx] == price.iloc[0:pv_range*2+1].min():\n",
    "                    data[i]['valleys'].iloc[idx] = price[idx]\n",
    "                    data[i]['pv'].iloc[idx] = 'valleys'\n",
    "            if price[idx] == price.iloc[idx-pv_range:idx+pv_range].max():\n",
    "                data[i]['peaks'].iloc[idx] = price[idx]\n",
    "                data[i]['pv'].iloc[idx] = 'peaks'\n",
    "            if price[idx] == price.iloc[idx-pv_range:idx+pv_range].min():\n",
    "                data[i]['valleys'].iloc[idx] = price[idx]\n",
    "                data[i]['pv'].iloc[idx] = 'valleys'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_pv_HL_function(data, pv_range):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    for i in data:\n",
    "        high = data[i]['High']\n",
    "        low = data[i]['Low']\n",
    "        data[i]['peaks'] = pd.Series(dtype='float64')\n",
    "        data[i]['valleys'] = pd.Series(dtype='float64')\n",
    "        data[i]['pv'] = pd.Series(dtype='str')\n",
    "        for idx in range(0, len(high)):\n",
    "            if idx < pv_range:\n",
    "                if high[idx] == high.iloc[0:pv_range*2+1].max():\n",
    "                    data[i]['peaks'].iloc[idx] = high[idx]\n",
    "                    data[i]['pv'].iloc[idx] = 'peaks'\n",
    "                if low[idx] == low.iloc[0:pv_range*2+1].min():\n",
    "                    data[i]['valleys'].iloc[idx] = low[idx]\n",
    "                    data[i]['pv'].iloc[idx] = 'valleys'\n",
    "            if high[idx] == high.iloc[idx-pv_range:idx+pv_range].max():\n",
    "                data[i]['peaks'].iloc[idx] = high[idx]\n",
    "                data[i]['pv'].iloc[idx] = 'peaks'\n",
    "            if low[idx] == low.iloc[idx-pv_range:idx+pv_range].min():\n",
    "                data[i]['valleys'].iloc[idx] = low[idx]\n",
    "                data[i]['pv'].iloc[idx] = 'valleys'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(train_data, test_data, pv_range, pv_method):\n",
    "    if pv_method == 'CL':\n",
    "        find_data_pv_CL_function(train_data, pv_range)\n",
    "        find_data_pv_CL_function(test_data, pv_range)\n",
    "    elif pv_method == 'HL':\n",
    "        find_data_pv_HL_function(train_data, pv_range)\n",
    "        find_data_pv_HL_function(test_data, pv_range)\n",
    "    else :\n",
    "            print('worng pv_method')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_polynomial_function(data_stock, order_lower_limit, order_upper_limit):\n",
    "    warnings.simplefilter('ignore', np.RankWarning)\n",
    "    processed_signal = {}\n",
    "    for i in data_stock:\n",
    "        processed_signal[i] = {}\n",
    "        data = data_stock[i]['Close']\n",
    "        y = np.array(data)\n",
    "        x = np.arange(0, y.size)\n",
    "        for order in range(order_lower_limit, order_upper_limit+1):\n",
    "            coefficient  = np.polyfit(x, y, order)\n",
    "            polynomial = np.polyval(coefficient, x)\n",
    "            processed_signal[i][order] = pd.DataFrame(\n",
    "                {'Close': np.tile(polynomial, 2)})   \n",
    "    return processed_signal\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_harmonics_function(data_stock, data_range):\n",
    "    harmonics = {}\n",
    "    for i in data_stock:\n",
    "        harmonics[i] = {}\n",
    "        # get data_stock's infomation\n",
    "        data = data_stock[i]['Close']\n",
    "        array_data = np.array(data)\n",
    "        n_data = array_data.size\n",
    "        time_data = np.arange(0, n_data)\n",
    "\n",
    "        # detrend data\n",
    "        # find linear trend in data\n",
    "        Polynomial = np.polyfit(time_data, array_data, 1)\n",
    "        data_notrend = array_data - Polynomial[0] * time_data    # detrended x\n",
    "\n",
    "        # fft process\n",
    "        data_freqdom = fft.fft(data_notrend, n=n_data)\n",
    "        frequence = fft.fftfreq(n=n_data, d=1)\n",
    "        f_positive = frequence[np.where(frequence > 0)]\n",
    "        data_freqdom_positive = data_freqdom[np.where(frequence > 0)]\n",
    "\n",
    "        # sort indexes\n",
    "        indexes = list(range(f_positive.size))      # frequencies\n",
    "        # sort method 1\n",
    "        # indexes.sort(key = lambda i: np.absolute(frequence[i]))     # sort indexes by frequency, lower -> higher\n",
    "        # sort method 2 :\n",
    "        # sort indexes by amplitudes, lower -> higher\n",
    "        indexes.sort(key=lambda i: np.absolute(data_freqdom[i]))\n",
    "        indexes.reverse()       # sort indexes by amplitudes, higher -> lower\n",
    "\n",
    "        # get data_all_time'size\n",
    "        time_transfer = np.arange(0, data_range*2)\n",
    "\n",
    "        # mix harmonics\n",
    "        for j in indexes:\n",
    "            ampli = np.absolute(\n",
    "                data_freqdom_positive[j]) / n_data     # amplitude\n",
    "            phase = np.angle(data_freqdom_positive[j])      # phase\n",
    "            harmonics[i][j] = ampli * \\\n",
    "                np.cos(2 * np.pi * f_positive[j] * time_transfer + phase)\n",
    "    return harmonics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_harmonics_function(harmonics, n_harm_lower_limit, n_harm_upper_limit):\n",
    "    processed_signal = {}\n",
    "    for i in harmonics:\n",
    "        processed_signal[i] = {}\n",
    "        for n_harm in range(n_harm_lower_limit, n_harm_upper_limit+1):\n",
    "            mixed_harmonic = np.zeros(len(harmonics[i][0]))\n",
    "            for j in range(n_harm):\n",
    "                mixed_harmonic += harmonics[i][j]\n",
    "            processed_signal[i][n_harm] = pd.DataFrame(\n",
    "                {'Close': mixed_harmonic})\n",
    "    return processed_signal\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signal processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signal_pv_function(signal, pv_range):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    for i in signal:\n",
    "        for j in signal[i]:\n",
    "            data = signal[i][j]['Close']\n",
    "            signal[i][j]['peaks'] = pd.Series(dtype='float64')\n",
    "            signal[i][j]['valleys'] = pd.Series(dtype='float64')\n",
    "            signal[i][j]['pv'] = pd.Series(dtype='str')\n",
    "            for idx in range(0, len(data)):\n",
    "                if idx < pv_range:\n",
    "                    if data[idx] == data.iloc[0:pv_range*2+1].max():\n",
    "                        signal[i][j]['peaks'].iloc[idx] = data[idx]\n",
    "                        signal[i][j]['pv'].iloc[idx] = 'peaks'\n",
    "                    if data[idx] == data.iloc[0:pv_range*2+1].min():\n",
    "                        signal[i][j]['valleys'].iloc[idx] = data[idx]\n",
    "                        signal[i][j]['pv'].iloc[idx] = 'valleys'\n",
    "                if data[idx] == data.iloc[idx-pv_range:idx+pv_range].max():\n",
    "                    signal[i][j]['peaks'].iloc[idx] = data[idx]\n",
    "                    signal[i][j]['pv'].iloc[idx] = 'peaks'\n",
    "                if data[idx] == data.iloc[idx-pv_range:idx+pv_range].min():\n",
    "                    signal[i][j]['valleys'].iloc[idx] = data[idx]\n",
    "                    signal[i][j]['pv'].iloc[idx] = 'valleys'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signal_lead_train_function(data, processed_signal):\n",
    "    for d in data:\n",
    "        for p in processed_signal[d]:\n",
    "            # processed_signal[d][p]['pv'] = pd.Series(dtype='str')\n",
    "            processing_signal = processed_signal[d][p].head(len(data[d]))\n",
    "            p_data = pd.DataFrame(\n",
    "                {'peaks': data[d]['peaks'], 'count': range(len(data[d]))})\n",
    "            p_data = p_data.drop(p_data[p_data['peaks'].isna()].index)\n",
    "            p_data_count = list(p_data['count'])\n",
    "            p_signal = pd.DataFrame(\n",
    "                {'peaks': processing_signal['peaks'], 'count': range(len(processing_signal))})\n",
    "            p_signal = p_signal.drop(p_signal[p_signal['peaks'].isna()].index)\n",
    "            p_signal_list = list(p_signal['count'])\n",
    "            p_lead = []\n",
    "            for i in range(0, len(p_signal_list)):\n",
    "                temp = []\n",
    "                temp_abs = []\n",
    "                temp_2 = []\n",
    "                for j in range(0, len(p_data_count)):\n",
    "                    temp.append((p_data_count[j] - p_signal_list[i]))\n",
    "                    temp_abs.append(abs(p_data_count[j] - p_signal_list[i]))\n",
    "                for k in range(0, len(temp_abs)):\n",
    "                    if temp_abs[k] == min(temp_abs):\n",
    "                        temp_2 = temp[k]\n",
    "                p_lead.append(temp_2)\n",
    "            p_signal['lead'] = p_lead\n",
    "\n",
    "            v_data = pd.DataFrame(\n",
    "                {'valleys': data[d]['valleys'], 'count': range(len(data[d]))})\n",
    "            v_data = v_data.drop(v_data[v_data['valleys'].isna()].index)\n",
    "            v_data_count = list(v_data['count'])\n",
    "            v_signal = pd.DataFrame(\n",
    "                {'valleys': processing_signal['valleys'], 'count': range(len(processing_signal))})\n",
    "            v_signal = v_signal.drop(\n",
    "                v_signal[v_signal['valleys'].isna()].index)\n",
    "            v_signal_list = list(v_signal['count'])\n",
    "            v_lead = []\n",
    "            for i in range(0, len(v_signal_list)):\n",
    "                temp = []\n",
    "                temp_abs = []\n",
    "                temp_2 = []\n",
    "                for j in range(0, len(v_data_count)):\n",
    "                    temp.append((v_data_count[j] - v_signal_list[i]))\n",
    "                    temp_abs.append(abs(v_data_count[j] - v_signal_list[i]))\n",
    "                for k in range(0, len(temp_abs)):\n",
    "                    if temp_abs[k] == min(temp_abs):\n",
    "                        temp_2 = temp[k]\n",
    "                v_lead.append(temp_2)\n",
    "            v_signal['lead'] = v_lead\n",
    "\n",
    "            processed_signal[d][p]['lead'] = pd.Series(dtype='float64')\n",
    "            processed_signal[d][p]['lead'].loc[p_signal['lead'].index] = p_signal['lead']\n",
    "            processed_signal[d][p]['lead'].loc[v_signal['lead'].index] = v_signal['lead']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train_data, n_harm_lower_limit, n_harm_upper_limit, pv_range, data_range):\n",
    "    harmonics = data_to_harmonics_function(train_data, data_range)\n",
    "    processed_signal = mix_harmonics_function(\n",
    "        harmonics, n_harm_lower_limit, n_harm_upper_limit)\n",
    "    find_signal_pv_function(processed_signal, pv_range)\n",
    "    find_signal_lead_train_function(train_data, processed_signal)\n",
    "    return harmonics, processed_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_2(train_data, order_lower_limit, order_upper_limit, pv_range, data_range):\n",
    "    processed_signal = data_to_polynomial_function(train_data, order_lower_limit, order_upper_limit)\n",
    "    find_signal_pv_function(processed_signal, pv_range)\n",
    "    find_signal_lead_train_function(train_data, processed_signal)\n",
    "    return processed_signal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fit_error_function(processed_signal, fit_method):\n",
    "    errors = {}\n",
    "    error = []\n",
    "    for i in processed_signal:\n",
    "        errors[i] = {}\n",
    "        for j in processed_signal[i]:\n",
    "            signal_dropna = processed_signal[i][j].drop(\n",
    "                processed_signal[i][j][processed_signal[i][j]['lead'].isna()].index)\n",
    "            if fit_method == 'mean':\n",
    "                error = signal_dropna['lead'].mean()\n",
    "            elif fit_method == 'abs':\n",
    "                error = abs(signal_dropna['lead']).mean()\n",
    "            elif fit_method == 'rmse':\n",
    "                mse = np.square(np.subtract(np.zeros_like(\n",
    "                    signal_dropna['lead']), signal_dropna['lead'])).mean()\n",
    "                rmse = math.sqrt(mse)\n",
    "                error = rmse\n",
    "            else :\n",
    "                print('worng fit_method')\n",
    "            errors[i][j] = error\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_fit_harm_function(processed_signal, errors):\n",
    "    best_error = {}\n",
    "    best_fit_harm = {}\n",
    "    for i in processed_signal:\n",
    "        best_error[i] = pd.Series(errors[i]).abs().min()\n",
    "        best_fit_harm[i] = pd.Series(errors[i]).abs().idxmin()\n",
    "    return best_fit_harm, best_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signal_lead_test_function(data, processed_signal):\n",
    "    for d in data:\n",
    "        for p in processed_signal[d]:\n",
    "            processed_signal[d][p]['pv'] = pd.Series(dtype='str')\n",
    "            processing_signal = processed_signal[d][p].tail(len(data[d]))\n",
    "            p_data = pd.DataFrame(\n",
    "                {'peaks': data[d]['peaks'], 'count': range(len(data[d]))})\n",
    "            p_data = p_data.drop(p_data[p_data['peaks'].isna()].index)\n",
    "            p_data_count = list(p_data['count'])\n",
    "            p_signal = pd.DataFrame(\n",
    "                {'peaks': processing_signal['peaks'], 'count': range(len(processing_signal))})\n",
    "            p_signal = p_signal.drop(p_signal[p_signal['peaks'].isna()].index)\n",
    "            p_signal_list = list(p_signal['count'])\n",
    "            p_lead = []\n",
    "            for i in range(0, len(p_signal_list)):\n",
    "                temp = []\n",
    "                temp_abs = []\n",
    "                temp_2 = []\n",
    "                for j in range(0, len(p_data_count)):\n",
    "                    temp.append((p_data_count[j] - p_signal_list[i]))\n",
    "                    temp_abs.append(abs(p_data_count[j] - p_signal_list[i]))\n",
    "                for k in range(0, len(temp_abs)):\n",
    "                    if temp_abs[k] == min(temp_abs):\n",
    "                        temp_2 = temp[k]\n",
    "                p_lead.append(temp_2)\n",
    "            p_signal['lead'] = p_lead\n",
    "\n",
    "            v_data = pd.DataFrame(\n",
    "                {'valleys': data[d]['valleys'], 'count': range(len(data[d]))})\n",
    "            v_data = v_data.drop(v_data[v_data['valleys'].isna()].index)\n",
    "            v_data_count = list(v_data['count'])\n",
    "            v_signal = pd.DataFrame(\n",
    "                {'valleys': processing_signal['valleys'], 'count': range(len(processing_signal))})\n",
    "            v_signal = v_signal.drop(\n",
    "                v_signal[v_signal['valleys'].isna()].index)\n",
    "            v_signal_list = list(v_signal['count'])\n",
    "            v_lead = []\n",
    "            for i in range(0, len(v_signal_list)):\n",
    "                temp = []\n",
    "                temp_abs = []\n",
    "                temp_2 = []\n",
    "                for j in range(0, len(v_data_count)):\n",
    "                    temp.append((v_data_count[j] - v_signal_list[i]))\n",
    "                    temp_abs.append(abs(v_data_count[j] - v_signal_list[i]))\n",
    "                for k in range(0, len(temp_abs)):\n",
    "                    if temp_abs[k] == min(temp_abs):\n",
    "                        temp_2 = temp[k]\n",
    "                v_lead.append(temp_2)\n",
    "            v_signal['lead'] = v_lead\n",
    "\n",
    "            processed_signal[d][p]['lead'] = pd.Series(dtype='float64')\n",
    "            processed_signal[d][p]['lead'].loc[p_signal['lead'].index] = p_signal['lead']\n",
    "            processed_signal[d][p]['pv'].loc[p_signal['lead'].index] = 'peak'\n",
    "            processed_signal[d][p]['lead'].loc[v_signal['lead'].index] = v_signal['lead']\n",
    "            processed_signal[d][p]['pv'].loc[v_signal['lead'].index] = 'valley'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_lead_function(processed_signal, best_fit_harm):\n",
    "    first_date = {}\n",
    "    lead = {}\n",
    "    pv = {}\n",
    "    for i in processed_signal:\n",
    "        harm = best_fit_harm[i]\n",
    "        temp = processed_signal[i][harm].loc[list(\n",
    "            processed_signal[i][harm]['lead'].dropna().index)[0]]\n",
    "        first_date[i] = list(processed_signal[i][harm]\n",
    "                             ['lead'].dropna().index)[0]\n",
    "        lead[i] = temp['lead']\n",
    "        pv[i] = temp['pv']\n",
    "    return first_date, lead, pv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(test_data, processed_signal, fit_method):\n",
    "    errors = get_fit_error_function(processed_signal, fit_method)\n",
    "    best_fit_harm, best_error = get_best_fit_harm_function(\n",
    "        processed_signal, errors)\n",
    "    find_signal_lead_test_function(test_data, processed_signal)\n",
    "    first_date, lead, pv = get_first_lead_function(\n",
    "        processed_signal, best_fit_harm)\n",
    "    return errors, best_fit_harm, best_error, first_date, lead, pv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_result_table_function(processed_signal, test_data_start_list, lead, pv, best_error, best_fit_harm):\n",
    "    result_table = pd.DataFrame(columns=[\n",
    "        's_date', 't_date', 'lead', 'ans_date', 'pv', 'error', 'best_fit'])\n",
    "    for i in processed_signal:\n",
    "        result_table.loc[i, 'error'] = round(best_error[i], 2)\n",
    "        result_table.loc[i, 'best_fit'] = best_fit_harm[i]\n",
    "        result_table.loc[i, 'lead'] = lead[i]\n",
    "        result_table.loc[i, 'pv'] = pv[i]\n",
    "    result_table['s_date'] = test_data_start_list\n",
    "    return result_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_table_process_function(result_table, all_data, first_date, data_range):\n",
    "    for i in result_table.index:\n",
    "        t_date = all_data.iloc[all_data.index.get_loc(\n",
    "            result_table.loc[i, 's_date']) + first_date[i] - data_range].name\n",
    "        t_date = datetime.datetime.strftime(t_date, '%Y-%m-%d')\n",
    "        result_table.loc[i, 't_date'] = t_date\n",
    "        ans = all_data.iloc[int(all_data.index.get_loc(\n",
    "            result_table.loc[i, 't_date']) + result_table.loc[i, 'lead'])].name\n",
    "        ans = datetime.datetime.strftime(ans, '%Y-%m-%d')\n",
    "        result_table.loc[i, 'ans_date'] = ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_error_function(result_table):\n",
    "    final_error = round(\n",
    "        sum([abs(ele) for ele in result_table['lead']]) / len(result_table['lead']), 2)\n",
    "    return final_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(processed_signal, test_data_start_list, all_data, best_fit_harm, best_error, first_date, lead, pv, data_range):\n",
    "    result_table = built_result_table_function(\n",
    "        processed_signal, test_data_start_list, lead, pv, best_error, best_fit_harm)\n",
    "    result_table_process_function(\n",
    "        result_table, all_data, first_date, data_range)\n",
    "    final_error = compute_final_error_function(result_table)\n",
    "    return result_table, final_error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_pv_function(data, pv_range):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    price = data['Close']\n",
    "    data['peaks'] = pd.Series(dtype='float64')\n",
    "    data['valleys'] = pd.Series(dtype='float64')\n",
    "    data['pv'] = pd.Series(dtype='str')\n",
    "    for idx in range(0, len(price)):\n",
    "        if idx < pv_range:\n",
    "            if price[idx] == price.iloc[0:pv_range*2+1].max():\n",
    "                data['peaks'].iloc[idx] = price[idx]\n",
    "                data['pv'].iloc[idx] = 'peaks'\n",
    "            if price[idx] == price.iloc[0:pv_range*2+1].min():\n",
    "                data['valleys'].iloc[idx] = price[idx]\n",
    "                data['pv'].iloc[idx] = 'valleys'\n",
    "        if price[idx] == price.iloc[idx-pv_range:idx+pv_range].max():\n",
    "            data['peaks'].iloc[idx] = price[idx]\n",
    "            data['pv'].iloc[idx] = 'peaks'\n",
    "        if price[idx] == price.iloc[idx-pv_range:idx+pv_range].min():\n",
    "            data['valleys'].iloc[idx] = price[idx]\n",
    "            data['pv'].iloc[idx] = 'valleys'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_plot_result_table(test_data, all_data, result_table, pv_range):\n",
    "    date_list = sorted(\n",
    "        list(result_table['s_date']) + list(result_table['t_date']) + list(result_table['ans_date']))\n",
    "    all_index = all_data.loc[date_list[0]:date_list[-1]].index\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(30, 8), sharex=True)\n",
    "    for d in test_data :\n",
    "        test_data[d].index = test_data[d]['Date']\n",
    "        test_data[d]= test_data[d].drop(test_data[d][test_data[d].index > date_list[-1]].index)\n",
    "        axes[0].plot(test_data[d].index, test_data[d]['Close'], 'gray', label='data', linewidth=3)\n",
    "    for d in test_data :\n",
    "        axes[0].plot(test_data[d].index, test_data[d]['peaks'],\n",
    "                    '^', c='royalblue', label='peaks')\n",
    "        axes[0].plot(test_data[d].index, test_data[d]['valleys'], 'v',\n",
    "                    c='orangered', label='valleys')\n",
    "    plot_model = pd.DataFrame(index=all_index, columns=[\n",
    "                                's_date', 't_date', 'ans_date', 'lead', 'pv'])\n",
    "    plot_model['s_date'].loc[result_table['s_date']] = True\n",
    "    plot_model['t_date'].loc[result_table['t_date']] = True\n",
    "    plot_model['lead'].loc[result_table['t_date']] = list(result_table['lead'])\n",
    "    plot_model['pv'].loc[result_table['t_date']] = list(result_table['pv'])\n",
    "    plot_model['ans_date'].loc[result_table['ans_date']] = True\n",
    "    for i, label in enumerate(plot_model['lead']):\n",
    "        if plot_model['pv'][i] == 'peak':\n",
    "            # axes[0].plot(plot_data.index[i], plot_data['Close'].iloc[i], '|',\n",
    "            #             c='red')\n",
    "            axes[1].plot(plot_model.index[i], plot_model['lead'][i], '^',\n",
    "                            c='royalblue')\n",
    "            text = str(label)\n",
    "            axes[1].annotate(text, (plot_model.index[i],\n",
    "                                plot_model['lead'][i]), fontsize=14)\n",
    "        elif plot_model['pv'][i] == 'valley':\n",
    "            # axes[0].plot(plot_data.index[i], plot_data['Close'].iloc[i], '|',\n",
    "            #             c='red')\n",
    "            axes[1].plot(plot_model.index[i], plot_model['lead'][i], 'v',\n",
    "                            c='orangered')\n",
    "            text = str(label)\n",
    "            axes[1].annotate(text, (plot_model.index[i],\n",
    "                                plot_model['lead'][i]), fontsize=14)\n",
    "\n",
    "    axes[0].set_ylabel(\"Stock price\", fontsize=14)\n",
    "    axes[0].grid(True)\n",
    "    axes[1].grid(True)\n",
    "    axes[1].set_ylabel(\"lead\", fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_funtion(\n",
    "    stock_name, date_predict_start, data_range, slide_range,\n",
    "        n_slide, pv_range, n_harm_lower_limit, n_harm_upper_limit, fit_method, pv_method):\n",
    "\n",
    "    # 1. Load data\n",
    "    train_data, test_data, all_data, test_data_start_list = load_data(\n",
    "        stock_name, date_predict_start, data_range, slide_range, n_slide)\n",
    "    # 2. Preprocessing\n",
    "    preprocessing(train_data, test_data, pv_range, pv_method)\n",
    "    # 3. Build model\n",
    "    harmonics, model = build_model(\n",
    "        train_data, n_harm_lower_limit, n_harm_upper_limit, pv_range, data_range)\n",
    "    # 4. Select model\n",
    "    errors, best_fit_harm, best_error, first_date, lead, pv = select_model(test_data,\n",
    "                                                                           model, fit_method)\n",
    "    # 5. Evaluate model\n",
    "    result_table, final_error = evaluate_model(\n",
    "        model, test_data_start_list, all_data, best_fit_harm, best_error, first_date, lead, pv, data_range)\n",
    "    print('final_error = ', final_error)\n",
    "    with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "        print(result_table)\n",
    "    draw_plot_result_table(test_data, all_data, result_table, pv_range)\n",
    "    return harmonics, model, errors, best_fit_harm, best_error, first_date, lead, pv, result_table, final_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_funtion_polynomial(\n",
    "    stock_name, date_predict_start, data_range, slide_range,\n",
    "        n_slide, pv_range, order_lower_limit, order_upper_limit, fit_method, pv_method):\n",
    "\n",
    "    # 1. Load data\n",
    "    train_data, test_data, all_data, test_data_start_list = load_data(\n",
    "    stock_name, date_predict_start, data_range, slide_range, n_slide)\n",
    "    # 2. Preprocessing\n",
    "    preprocessing(train_data, test_data, pv_range, pv_method)\n",
    "    # 3. Build model\n",
    "    model = build_model_2(\n",
    "        train_data, order_lower_limit, order_upper_limit, pv_range, data_range)\n",
    "    # 4. Select model\n",
    "    errors, best_fit_harm, best_error, first_date, lead, pv = select_model(test_data, \n",
    "        model, fit_method)\n",
    "    # 5. Evaluate model\n",
    "    result_table, final_error = evaluate_model(\n",
    "        model, test_data_start_list, all_data, best_fit_harm, best_error, first_date, lead, pv, data_range)\n",
    "    print('final_error = ', final_error)\n",
    "    with pd.option_context('display.max_rows', None,\n",
    "                        'display.max_columns', None,\n",
    "                        'display.precision', 3,\n",
    "                        ):\n",
    "        print(result_table)\n",
    "    draw_plot_result_table(test_data, all_data, result_table, pv_range)\n",
    "    return model, errors, best_fit_harm, best_error, first_date, lead, pv, result_table, final_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Close\n",
       "0      1\n",
       "1      2\n",
       "2      3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Close':[1,2,3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_arima_function(train_data):\n",
    "    predictions = {}\n",
    "    for i in train_data:\n",
    "        train_ar = train_data[i]['Open'].values\n",
    "        history = [x for x in train_ar]\n",
    "        prediction = list()\n",
    "        for t in range(len(train_ar)):\n",
    "            model = sm.tsa.arima.ARIMA(history, order=(5,1,0))\n",
    "            model_fit = model.fit()\n",
    "            output = model_fit.forecast()\n",
    "            yhat = output[0]\n",
    "            prediction.append(yhat)\n",
    "            # obs = test_ar[t]\n",
    "            history.append(yhat)\n",
    "            # print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "        predictions[i] = pd.DataFrame({'Close':prediction})\n",
    "        # error = mean_squared_error(test_ar, predictions)\n",
    "        # print('Testing Mean Squared Error: %.3f' % error)\n",
    "        # error2 = smape_kun(test_ar, predictions)\n",
    "        # print('Symmetric mean absolute percentage error: %.3f' % error2)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = \"^GSPC\"\n",
    "date_predict_start = '2020-01-01'\n",
    "data_range = 200\n",
    "slide_range = 10\n",
    "n_slide = 2\n",
    "pv_range = 2\n",
    "order_lower_limit = 0\n",
    "order_upper_limit = 50\n",
    "fit_method = 'rmse'\n",
    "pv_method = 'HL'\n",
    "train_data, test_data, all_data, test_data_start_list = load_data(\n",
    "    stock_name, date_predict_start, data_range, slide_range, n_slide)\n",
    "# 2. Preprocessing\n",
    "preprocessing(train_data, test_data, pv_range, pv_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_0':           Date         Open         High          Low        Close  \\\n",
       " 0   2019-03-19  2840.760010  2852.419922  2823.270020  2832.570068   \n",
       " 1   2019-03-20  2831.340088  2843.540039  2812.429932  2824.229980   \n",
       " 2   2019-03-21  2819.719971  2860.310059  2817.379883  2854.879883   \n",
       " 3   2019-03-22  2844.520020  2846.159912  2800.469971  2800.709961   \n",
       " 4   2019-03-25  2796.010010  2809.790039  2785.020020  2798.360107   \n",
       " ..         ...          ...          ...          ...          ...   \n",
       " 195 2019-12-24  3225.449951  3226.429932  3220.510010  3223.379883   \n",
       " 196 2019-12-26  3227.199951  3240.080078  3227.199951  3239.909912   \n",
       " 197 2019-12-27  3247.229980  3247.929932  3234.370117  3240.020020   \n",
       " 198 2019-12-30  3240.090088  3240.919922  3216.570068  3221.290039   \n",
       " 199 2019-12-31  3215.179932  3231.719971  3212.030029  3230.780029   \n",
       " \n",
       "          Volume  Dividends  Stock Splits        peaks      valleys       pv  \n",
       " 0    3650740000          0             0          NaN          NaN      NaN  \n",
       " 1    3779160000          0             0          NaN          NaN      NaN  \n",
       " 2    3612620000          0             0  2860.310059          NaN    peaks  \n",
       " 3    4253730000          0             0          NaN          NaN      NaN  \n",
       " 4    3406110000          0             0          NaN  2785.020020  valleys  \n",
       " ..          ...        ...           ...          ...          ...      ...  \n",
       " 195  1296530000          0             0          NaN          NaN      NaN  \n",
       " 196  2164540000          0             0          NaN          NaN      NaN  \n",
       " 197  2429150000          0             0  3247.929932          NaN    peaks  \n",
       " 198  3021720000          0             0          NaN          NaN      NaN  \n",
       " 199  2894760000          0             0          NaN  3212.030029  valleys  \n",
       " \n",
       " [200 rows x 11 columns],\n",
       " 'data_1':           Date         Open         High          Low        Close  \\\n",
       " 0   2019-04-02  2868.239990  2872.899902  2858.750000  2867.239990   \n",
       " 1   2019-04-03  2876.090088  2885.250000  2865.169922  2873.399902   \n",
       " 2   2019-04-04  2873.989990  2881.280029  2867.139893  2879.389893   \n",
       " 3   2019-04-05  2884.159912  2893.239990  2882.989990  2892.739990   \n",
       " 4   2019-04-08  2888.459961  2895.949951  2880.780029  2895.770020   \n",
       " ..         ...          ...          ...          ...          ...   \n",
       " 195 2020-01-09  3266.030029  3275.580078  3263.669922  3274.699951   \n",
       " 196 2020-01-10  3281.810059  3282.989990  3260.860107  3265.350098   \n",
       " 197 2020-01-13  3271.129883  3288.129883  3268.429932  3288.129883   \n",
       " 198 2020-01-14  3285.350098  3294.250000  3277.189941  3283.149902   \n",
       " 199 2020-01-15  3282.270020  3298.659912  3280.689941  3289.290039   \n",
       " \n",
       "          Volume  Dividends  Stock Splits        peaks  valleys       pv  \n",
       " 0    3267210000          0             0          NaN  2858.75  valleys  \n",
       " 1    3570000000          0             0          NaN      NaN      NaN  \n",
       " 2    3038740000          0             0          NaN      NaN      NaN  \n",
       " 3    3155970000          0             0          NaN      NaN      NaN  \n",
       " 4    3056460000          0             0  2895.949951      NaN    peaks  \n",
       " ..          ...        ...           ...          ...      ...      ...  \n",
       " 195  3641230000          0             0          NaN      NaN      NaN  \n",
       " 196  3214580000          0             0          NaN      NaN      NaN  \n",
       " 197  3459390000          0             0          NaN      NaN      NaN  \n",
       " 198  3687620000          0             0          NaN      NaN      NaN  \n",
       " 199  3721490000          0             0  3298.659912      NaN    peaks  \n",
       " \n",
       " [200 rows x 11 columns]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[39m=\u001b[39m data_to_arima_function(train_data)\n",
      "Cell \u001b[1;32mIn [55], line 9\u001b[0m, in \u001b[0;36mdata_to_arima_function\u001b[1;34m(train_data)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(train_ar)):\n\u001b[0;32m      8\u001b[0m     model \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39mtsa\u001b[39m.\u001b[39marima\u001b[39m.\u001b[39mARIMA(history, order\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m     model_fit \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit()\n\u001b[0;32m     10\u001b[0m     output \u001b[39m=\u001b[39m model_fit\u001b[39m.\u001b[39mforecast()\n\u001b[0;32m     11\u001b[0m     yhat \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\statsmodels\\tsa\\arima\\model.py:390\u001b[0m, in \u001b[0;36mARIMA.fit\u001b[1;34m(self, start_params, transformed, includes_fixed, method, method_kwargs, gls, gls_kwargs, cov_type, cov_kwds, return_params, low_memory)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     method_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mdisp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m--> 390\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    391\u001b[0m         return_params\u001b[39m=\u001b[39;49mreturn_params, low_memory\u001b[39m=\u001b[39;49mlow_memory,\n\u001b[0;32m    392\u001b[0m         cov_type\u001b[39m=\u001b[39;49mcov_type, cov_kwds\u001b[39m=\u001b[39;49mcov_kwds, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmethod_kwargs)\n\u001b[0;32m    393\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_params:\n\u001b[0;32m    394\u001b[0m         res\u001b[39m.\u001b[39mfit_details \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mmlefit\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:728\u001b[0m, in \u001b[0;36mMLEModel.fit\u001b[1;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    727\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth\n\u001b[1;32m--> 728\u001b[0m res \u001b[39m=\u001b[39m func(mlefit\u001b[39m.\u001b[39;49mparams, transformed\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, includes_fixed\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    729\u001b[0m            cov_type\u001b[39m=\u001b[39;49mcov_type, cov_kwds\u001b[39m=\u001b[39;49mcov_kwds)\n\u001b[0;32m    731\u001b[0m res\u001b[39m.\u001b[39mmlefit \u001b[39m=\u001b[39m mlefit\n\u001b[0;32m    732\u001b[0m res\u001b[39m.\u001b[39mmle_retvals \u001b[39m=\u001b[39m mlefit\u001b[39m.\u001b[39mmle_retvals\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:889\u001b[0m, in \u001b[0;36mMLEModel.smooth\u001b[1;34m(self, params, transformed, includes_fixed, complex_step, cov_type, cov_kwds, return_ssm, results_class, results_wrapper_class, **kwargs)\u001b[0m\n\u001b[0;32m    886\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssm\u001b[39m.\u001b[39msmooth(complex_step\u001b[39m=\u001b[39mcomplex_step, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39m# Wrap in a results object\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_results(params, result, return_ssm, cov_type,\n\u001b[0;32m    890\u001b[0m                           cov_kwds, results_class,\n\u001b[0;32m    891\u001b[0m                           results_wrapper_class)\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:788\u001b[0m, in \u001b[0;36mMLEModel._wrap_results\u001b[1;34m(self, params, result, return_raw, cov_type, cov_kwds, results_class, wrapper_class)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[39mif\u001b[39;00m wrapper_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    786\u001b[0m         wrapper_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_res_classes[\u001b[39m'\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 788\u001b[0m     res \u001b[39m=\u001b[39m results_class(\u001b[39mself\u001b[39;49m, params, result, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresult_kwargs)\n\u001b[0;32m    789\u001b[0m     result \u001b[39m=\u001b[39m wrapper_class(res)\n\u001b[0;32m    790\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:1806\u001b[0m, in \u001b[0;36mSARIMAXResults.__init__\u001b[1;34m(self, model, params, filter_results, cov_type, **kwargs)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, params, filter_results, cov_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1805\u001b[0m              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 1806\u001b[0m     \u001b[39msuper\u001b[39;49m(SARIMAXResults, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, params, filter_results,\n\u001b[0;32m   1807\u001b[0m                                          cov_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf_resid \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minf  \u001b[39m# attribute required for wald tests\u001b[39;00m\n\u001b[0;32m   1811\u001b[0m     \u001b[39m# Save _init_kwds\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:2337\u001b[0m, in \u001b[0;36mMLEResults.__init__\u001b[1;34m(self, model, params, results, cov_type, cov_kwds, **kwargs)\u001b[0m\n\u001b[0;32m   2335\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2336\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rank \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2337\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_robustcov_results(cov_type\u001b[39m=\u001b[39;49mcov_type, use_self\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2338\u001b[0m                                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcov_kwds)\n\u001b[0;32m   2339\u001b[0m \u001b[39mexcept\u001b[39;00m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mLinAlgError:\n\u001b[0;32m   2340\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rank \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:2567\u001b[0m, in \u001b[0;36mMLEResults._get_robustcov_results\u001b[1;34m(self, cov_type, **kwargs)\u001b[0m\n\u001b[0;32m   2564\u001b[0m     res\u001b[39m.\u001b[39mcov_kwds[\u001b[39m'\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m descriptions[\u001b[39m'\u001b[39m\u001b[39mOIM\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2565\u001b[0m                                         approx_type\u001b[39m=\u001b[39mapprox_type_str)\n\u001b[0;32m   2566\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mopg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m-> 2567\u001b[0m     res\u001b[39m.\u001b[39mcov_params_default \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39;49mcov_params_opg\n\u001b[0;32m   2568\u001b[0m     res\u001b[39m.\u001b[39mcov_kwds[\u001b[39m'\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m descriptions[\u001b[39m'\u001b[39m\u001b[39mOPG\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2569\u001b[0m                                         approx_type\u001b[39m=\u001b[39mapprox_type_str)\n\u001b[0;32m   2570\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrobust\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrobust_oim\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\pandas\\_libs\\properties.pyx:37\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:2697\u001b[0m, in \u001b[0;36mMLEResults.cov_params_opg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2691\u001b[0m \u001b[39m@cache_readonly\u001b[39m\n\u001b[0;32m   2692\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcov_params_opg\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   2693\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2694\u001b[0m \u001b[39m    (array) The variance / covariance matrix. Computed using the outer\u001b[39;00m\n\u001b[0;32m   2695\u001b[0m \u001b[39m    product of gradients method.\u001b[39;00m\n\u001b[0;32m   2696\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2697\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cov_params_opg(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cov_approx_complex_step,\n\u001b[0;32m   2698\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cov_approx_centered)\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:2681\u001b[0m, in \u001b[0;36mMLEResults._cov_params_opg\u001b[1;34m(self, approx_complex_step, approx_centered)\u001b[0m\n\u001b[0;32m   2679\u001b[0m     neg_cov[mask] \u001b[39m=\u001b[39m tmp\n\u001b[0;32m   2680\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2681\u001b[0m     (neg_cov, singular_values) \u001b[39m=\u001b[39m pinv_extended(evaluated_hessian)\n\u001b[0;32m   2683\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams, transformed\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, includes_fixed\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   2684\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rank \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\statsmodels\\tools\\tools.py:339\u001b[0m, in \u001b[0;36mpinv_extended\u001b[1;34m(x, rcond)\u001b[0m\n\u001b[0;32m    337\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\n\u001b[0;32m    338\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mconjugate()\n\u001b[1;32m--> 339\u001b[0m u, s, vt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49msvd(x, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    340\u001b[0m s_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(s)\n\u001b[0;32m    341\u001b[0m m \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\e4903\\Desktop\\code\\venv_stock\\lib\\site-packages\\numpy\\linalg\\linalg.py:1657\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[0;32m   1654\u001b[0m         gufunc \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39msvd_n_s\n\u001b[0;32m   1656\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->DdD\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->ddd\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m-> 1657\u001b[0m u, s, vh \u001b[39m=\u001b[39m gufunc(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[0;32m   1658\u001b[0m u \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1659\u001b[0m s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mastype(_realType(result_t), copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = data_to_arima_function(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_3(train_data, pv_range):\n",
    "    processed_signal = data_to_arima_function(train_data)\n",
    "    # find_signal_pv_function(processed_signal, pv_range)\n",
    "    # find_signal_lead_train_function(train_data, processed_signal)\n",
    "    return processed_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build model\n",
    "# model = build_model_3(\n",
    "#     train_data, pv_range)\n",
    "processed_signal = data_to_arima_function(train_data)\n",
    "# find_signal_pv_function(processed_signal, pv_range)\n",
    "# find_signal_lead_train_function(train_data, processed_signal)\n",
    "# 4. Select model\n",
    "# errors, best_fit_harm, best_error, first_date, lead, pv = select_model(test_data, \n",
    "#     model, fit_method)\n",
    "# # 5. Evaluate model\n",
    "# result_table, final_error = evaluate_model(\n",
    "#     model, test_data_start_list, all_data, best_fit_harm, best_error, first_date, lead, pv, data_range)\n",
    "# print('final_error = ', final_error)\n",
    "# with pd.option_context('display.max_rows', None,\n",
    "#                        'display.max_columns', None,\n",
    "#                        'display.precision', 3,\n",
    "#                        ):\n",
    "#     print(result_table)\n",
    "# draw_plot_result_table(test_data, all_data, result_table, pv_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not numpy.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m find_signal_pv_function(processed_signal, pv_range)\n",
      "Cell \u001b[1;32mIn [11], line 5\u001b[0m, in \u001b[0;36mfind_signal_pv_function\u001b[1;34m(signal, pv_range)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m signal:\n\u001b[0;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m signal[i]:\n\u001b[1;32m----> 5\u001b[0m         data \u001b[39m=\u001b[39m signal[i][j][\u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m         signal[i][j][\u001b[39m'\u001b[39m\u001b[39mpeaks\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat64\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m         signal[i][j][\u001b[39m'\u001b[39m\u001b[39mvalleys\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat64\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not numpy.float64"
     ]
    }
   ],
   "source": [
    "find_signal_pv_function(processed_signal, pv_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_name = \"^GSPC\"\n",
    "# date_predict_start = '2019-06-01'\n",
    "# data_range = 50\n",
    "# slide_range = 10\n",
    "# n_slide = 70\n",
    "# pv_range = 2\n",
    "# order_lower_limit = 1\n",
    "# order_upper_limit = 50\n",
    "# fit_method = 'abs'\n",
    "# pv_method = 'CL'\n",
    "# model, errors, best_fit_harm, best_error, first_date, lead, pv, result_table, final_error = main_funtion_polynomial(\n",
    "#     stock_name, date_predict_start, data_range, slide_range,\n",
    "#     n_slide, pv_range, order_lower_limit, order_upper_limit, fit_method, pv_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_name = \"^GSPC\"\n",
    "# date_predict_start = '2019-06-01'\n",
    "# data_range = 100\n",
    "# slide_range = 10\n",
    "# n_slide = 1\n",
    "# pv_range = 2\n",
    "# n_harm_lower_limit = 1\n",
    "# n_harm_upper_limit = 49\n",
    "# fit_method = 'maen'\n",
    "# pv_method = 'CL'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv_stock': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba787645ba8ce3b43ca5d5c1bc5ea17dd580ada22e1bd31731a2a86b718f16cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
